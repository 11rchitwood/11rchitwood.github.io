---
title: "Data Science from Scratch"
date: "2025-03-17"
---

Is the R vs Python debate still alive in 2025?

I'm not sure, but I was digging into an old tome by [Joel Grus](https://joelgrus.com) entitled *Data Science from Scratch*. It's a great book that teaches data science by building everything from the ground up in Python: implementing gradient descent by hand, writing your own k-nearest neighbors classifier, even coding up basic linear algebra operations. The pedagogical value is clear—understanding the fundamentals makes you a better practitioner.

But here's the thing: **nobody actually does data science from scratch anymore.**

And that's not a criticism—it's progress. The same way modern web developers don't write their own HTTP parsers and database drivers, data scientists don't (and shouldn't) implement basic statistical functions from first principles for production work. We stand on the shoulders of giants, using battle-tested libraries that have been refined by thousands of contributors over decades.

This is where language design philosophy matters. Some languages are designed as general-purpose tools that can be extended for specific domains. Others are purpose-built for a particular domain from day one. Python is firmly in the first camp—it's an excellent general-purpose language that has been *extended* into data science through libraries like NumPy, Pandas, and scikit-learn. R, on the other hand, was designed from the ground up for statistical computing and data analysis.

What does this mean in practice? Let me translate the code from the book's introduction to R and show you. Rather than wax philosophical, I'll let the examples speak for themselves.

# Motivational Hypothetical: DataSciencester

DataSciencester is *the* social network for data scientists. Let's do some data science.

## Finding Key Connectors

First, we're given a list of user data. Each item in the list is a dictionary. Each dictionary has an ID and a name.

```{python}
users = [
    { "id": 0, "name": "Hero" },
    { "id": 1, "name": "Dunn" },
    { "id": 2, "name": "Sue" },
    { "id": 3, "name": "Chi" },
    { "id": 4, "name": "Thor" },
    { "id": 5, "name": "Clive" },
    { "id": 6, "name": "Hicks" },
    { "id": 7, "name": "Devin" },
    { "id": 8, "name": "Kate" },
    { "id": 9, "name": "Klein" }
]
```

Already, I've got some issues here. I don't think this data structure makes sense. Why are we dealing with integer IDs at all? It can't be for performance. We only have 10 users! Lets just use the string names directly.

```{r}
users <- c("Hero", "Dunn", "Sue", "Chi", "Thor", "Clive", "Hicks", "Devin", "Kate", "Klein")
```

Using a character vector makes the most sense to me here. We don't want integers because mathematical operations don't make sense here.

Next is the friendship data.

```{python}
friendship_pairs = [(0, 1), (0, 2), (1, 2), (1, 3), (2, 3), (3, 4), (4, 5), (5, 6), (5, 7), (6, 8), (7, 8), (8, 9)]
```

We've got a list of ID pairs. By the way, this data structure is called an [edge list](https://en.wikipedia.org/wiki/Edge_list). In R, it makes more sense to represent this as a two column matrix.

```{r}
edges <- c("Hero", "Dunn", "Hero", "Sue", "Dunn", "Sue", "Dunn", "Chi", "Sue", "Chi", "Chi", "Thor", "Thor", "Clive", "Clive", "Hicks", "Clive", "Devin", "Hicks", "Kate", "Devin", "Kate", "Kate", "Klein")
friendship_pairs <- matrix(
  data = edges,
  ncol = 2,
  byrow = TRUE
)
friendship_pairs
```

I'll praise Python for one thing so far: data structure literals. I don't need to print `friendship_pairs` in the Python code because that would just print out the actual code itself. Reminds me of Lisp. That's good.

But let me dote on my first love R too. Common data structures like matrices are built right in. No `import numpy as np` required. This is what I mean by "batteries included" for data science—the fundamental data structures you need for statistical computing and data analysis are part of the language itself, not third-party extensions. Matrices aren't an afterthought in R; they're a first-class citizen.

Moving on, the author expertly points out that a list of pairs is not the best way to work with the data. Neither is a matrix really. Let's transform the data.

```{python}
# Initialize the dict with an empty list for each user id:
friendships = {user["id"]: [] for user in users}

# And loop over the friendship pairs to populate it:
for i, j in friendship_pairs:
    friendships[i].append(j)  # Add j as a friend of user i
    friendships[j].append(i)  # Add i as a friend of user j

print(friendships)
```

The author chose a dictionary here for its fast look-ups. I agree. In R, the equivalent data structure is a list.

```{r}
compute_friendships <- function(user) {
  c(
    friendship_pairs[, 1][friendship_pairs[, 2] == user],
    friendship_pairs[, 2][friendship_pairs[, 1] == user]
  )
}
friendships <- lapply(users, compute_friendships)
names(friendships) <- users
friendships
```

We don't like for-loops in R. The `apply()` family of functions are much better. The next question is "What's the average number of connections?"

```{python}
def number_of_friends(user):
    """How many friends does _user_ have?"""
    user_id = user["id"]
    friend_ids = friendships[user_id]
    return len(friend_ids)

total_connections = sum(number_of_friends(user)
                        for user in users)        # 24


assert total_connections == 24

num_users = len(users)                            # length of the users list
avg_connections = total_connections / num_users   # 24 / 10 == 2.4


assert num_users == 10
assert avg_connections == 2.4
```

This is a one-liner in R.

```{r}
mean(lengths(friendships))
```

Next, we want sort from most to least friends.

```{python}
# Create a list (user_id, number_of_friends).
num_friends_by_id = [(user["id"], number_of_friends(user))
                     for user in users]

num_friends_by_id.sort(                                # Sort the list
       key=lambda id_and_friends: id_and_friends[1],   # by num_friends
       reverse=True)                                   # largest to smallest

# Each pair is (user_id, num_friends):
# [(1, 3), (2, 3), (3, 3), (5, 3), (8, 3),
#  (0, 2), (4, 2), (6, 2), (7, 2), (9, 1)]


assert num_friends_by_id[0][1] == 3     # several people have 3 friends
assert num_friends_by_id[-1] == (9, 1)  # user 9 has only 1 friend
```

Yet another one-liner in R.

```{r}
sort(lengths(friendships), decreasing = TRUE)
```

## Data Scientists You May Know

The next section is basically meanders a bit, so I'll skip the bad friend-of-a-friend implementation and go straight to the correct one.

```{python}
from collections import Counter                   # not loaded by default

def friends_of_friends(user):
    user_id = user["id"]
    return Counter(
        foaf_id
        for friend_id in friendships[user_id]     # For each of my friends,
        for foaf_id in friendships[friend_id]     # find their friends
        if foaf_id != user_id                     # who aren't me
        and foaf_id not in friendships[user_id]   # and aren't my friends.
    )

assert friends_of_friends(users[3]) == Counter({0: 2, 5: 1})
```

This one is bit more tricky in R, but it helps to break it down case by case.

```{r}
friends_of_friends <- function(user) {
  users_friends <- friendships[[user]]
  friendships[names(friendships) %in% users_friends] |> 
    lapply(\(friends_list) setdiff(friends_list, user)) |> 
    lapply(\(friends_list) setdiff(friends_list, users_friends)) |> 
    unlist() |> 
    table() |> 
    sort(decreasing = TRUE)
}

friends_of_friends("Chi")
```

It's easy to extract a parts of many objects in R using single brackets and `%in%` to match names. From there, a combination of `lapply()` and `setdiff()` make it easy to remove the user from the list as well as the user's direct friends. Next, `unlist()` is a mostly reliable way to reduce a list down to a vector. Finally, `table()` and `sort()` get us to our final result. By the way, I don't like the function name `friends_of_friends()`. I think something like `n_mutual_friends()` would be more clear.

I'm going to skip the next part about users' interests. It doesn't really go anywhere.

## Salaries and Experience

Next we're given some anonymous data on salaries (in dollars) and tenures (in years).

```{python}
salaries_and_tenures = [(83000, 8.7), (88000, 8.1),
                        (48000, 0.7), (76000, 6),
                        (69000, 6.5), (76000, 7.5),
                        (60000, 2.5), (83000, 10),
                        (48000, 1.9), (63000, 4.2)]
```

Again, data structures are everything here. This is a clear case for a data frame for me.

```{r}
salaries_and_tenures <- data.frame(
  salary = c(83000, 88000, 48000, 76000, 69000, 76000, 60000, 83000, 48000, 63000),
  tenure = c(8.7, 8.1, 0.7, 6, 6.5, 7.5, 2.5, 10, 1.9, 4.2)
)
salaries_and_tenures
```

Column names are so nice to have here. Sure, a two-column matrix would work too, but I prefer matrices to contain homogeneous data within. Salary and tenure are measured in different units, which makes a data frame preferred for me.

This is another example of R's "batteries included" philosophy. Data frames aren't just built into R—they're *central* to how R thinks about data. The `data.frame` class was part of R from very early on, because the language designers understood that data scientists need to work with heterogeneous tabular data. Python eventually got Pandas, which is excellent, but it's an add-on library that came along much later (2008 for Pandas vs 1993 for S, R's predecessor). R was designed with this use case in mind from day one.

Next, the author includes a plot, but with no code. I get it. He doesn't want introduce Matplotlib yet—that would require explaining a whole new library with its own API, object model, and learning curve.

But in R, we get plots built right in:

```{r}
plot(
  salary ~ tenure,
  data = salaries_and_tenures,
  main = "Salaries by Years Experience",
  xlab = "Years Experience",
  ylab = "Salary"
)
```

No imports. No setup. Just `plot()`. This is a fundamental difference in philosophy. Python's general-purpose nature means you need to bring in specialized libraries for specialized tasks. R's domain-specific design means the tools you need for statistical computing and data visualization are part of the language itself. And notice the formula syntax (`salary ~ tenure`)—that's not just a nice-to-have, it's a domain-specific language for expressing statistical relationships that's baked right into R.

Skipping the erroneous average salary by tenure piece, we move on to bucketing tenure.

```{python}
from collections import defaultdict

def tenure_bucket(tenure):
    if tenure < 2:
        return "less than two"
    elif tenure < 5:
        return "between two and five"
    else:
        return "more than five"

# Keys are tenure buckets, values are lists of salaries for that bucket.
salary_by_tenure_bucket = defaultdict(list)

for salary, tenure in salaries_and_tenures:
    bucket = tenure_bucket(tenure)
    salary_by_tenure_bucket[bucket].append(salary)

# Keys are tenure buckets, values are average salary for that bucket
average_salary_by_bucket = {
  tenure_bucket: sum(salaries) / len(salaries)
  for tenure_bucket, salaries in salary_by_tenure_bucket.items()
}

assert average_salary_by_bucket == {
    'between two and five': 61500.0,
    'less than two': 48000.0,
    'more than five': 79166.66666666667
}
```

This is another win for R, which contains a built-in function called `cut()` for this. Even better, R contains a default data type for this called a `factor`, which is similar to an `enum` in other programming languages, except factors are integer vectors under the hood.

```{r}
salaries_and_tenures$tenure_bucket <- cut(
  x = salaries_and_tenures$tenure,
  breaks = c(0, 2, 5, Inf),
  labels = c("less than two", "between two and five", "more than five"),
  right = FALSE,
  ordered_result = TRUE
)

aggregate(salary ~ tenure_bucket, data = salaries_and_tenures, mean)
```

Also, `aggregate()` is super powerful, if complicated. I'm going to skip the rest of the examples, since they aren't that interesting.

## The Point: Don't Reinvent the Wheel

Before we move on to a final example, let me emphasize the broader point here. Throughout these examples from *Data Science from Scratch*, we've seen Python code that implements basic operations from first principles—manually looping through data structures, writing custom functions for averaging, sorting with lambda functions. This is great for learning, but it's not how modern data science works.

In real data science work, you want to use well-tested, optimized tools that thousands of people have already debugged and refined. The question is: which language makes those tools more immediately accessible?

R's answer is to build them into the language itself. The result is that even "base R"—the language without any additional packages—gives you a remarkably complete toolkit for data analysis. You can read data, clean it, visualize it, model it, and report on it using only what ships with R.

This doesn't mean you should only use base R. As we'll see in the conclusion, the modern R ecosystem offers fantastic tools that go far beyond base R's capabilities. But the fact that the foundations are solid and built-in matters enormously. It means less conceptual overhead, fewer dependency conflicts, and more time spent analyzing data rather than configuring your environment.

## Diversion: igraph

Going back to the first section on the DataSciencester social network, it strikes me that there is a better tool for this analysis: [igraph](https://igraph.org). It's a collection of network analysis tools implemented in C with bindings for Mathematica, Python, and R.

This is a perfect example of what I mean about specialized tools. Rather than implementing graph algorithms from scratch (calculating betweenness centrality by hand would be tedious and error-prone), we use a library that's been optimized and tested by network science experts. Since the R bindings for igraph were developed first (and I'm biased towards R), we'll use that version.

Notice how naturally igraph fits into R's ecosystem. Because R already has the right data structures (matrices, vectors, data frames), igraph can focus on the network-specific functionality rather than building infrastructure.

```{r}
library(igraph)

g <- make_graph(edges = edges, directed = FALSE)
plot(g)
```

We can size each vertex according to degree.

```{r}
V(g)$size <- degree(g) * 10
plot(g, vertex.size = V(g)$size)
```

But actually betweenness tells more of the story for this graph.

```{r}
#| classes: preview-image
V(g)$size <- betweenness(g)
plot(g, vertex.size = V(g)$size)
```

## Conclusion

The Zen of Python will tell you that Python is "batteries included." I'm here to tell you that **R is "batteries included" specifically for data science.** R has built-in data frames and matrices. It has plotting built right in too, and you know I love that given this site's name. It's vectorized from the start and comes with lots of handy functional programming tools like `apply()` and the gang. And we didn't even fit a statistical model—those are built right in too, with `lm()`, `glm()`, `t.test()`, `cor.test()`, and dozens more statistical functions ready to use without a single `import` statement.

But here's where it gets really interesting: **this firm foundation has led to an explosion of highly ergonomic tools built on top of R's base capabilities.**

### The Modern R Ecosystem

When you have solid, well-designed foundations—proper data frames, integrated statistical functions, native plotting capabilities—it becomes much easier to build powerful abstractions on top. The R ecosystem has flourished precisely because it started with the right primitives for data science.

Consider the tidyverse, a collection of packages designed for data science with a consistent philosophy and API. Because R already had data frames as a first-class concept, the tidyverse could focus on making them more ergonomic rather than inventing them from scratch. The result? Tools like:

- **dplyr** for data manipulation: `filter()`, `mutate()`, `summarize()`, `group_by()`, all operating on data frames with a consistent, readable syntax
- **ggplot2** for data visualization: building on R's native plotting capabilities to create a grammar of graphics that's both powerful and intuitive
- **tidyr** for data reshaping: `pivot_longer()` and `pivot_wider()` make data tidying a breeze
- **purrr** for functional programming: extending R's built-in `apply()` family with more consistent behavior and better error messages

Or look at **data.table**, which extends R's built-in data frame concept to handle massive datasets with incredible performance. It's fast because it's built on R's understanding of columnar data structures, not fighting against the language.

For Bayesian statistics, we have **brms**, which provides a high-level interface to Stan but feels natural because R already has formula notation and built-in distributions. For machine learning, **tidymodels** provides a unified interface to hundreds of models while respecting R's statistical heritage.

### Why This Matters

When you start with the right primitives—when data frames, statistical functions, and plotting aren't bolted on but are fundamental to the language—you create a virtuous cycle. Package developers can build on solid foundations rather than working around limitations. Users can learn new packages more easily because they share common conventions. And the entire ecosystem moves forward together.

This is why, despite the rise of Python in data science, R continues to be the tool of choice for statisticians, data analysts, and scientists who need to do serious statistical work. You can be productive immediately with base R, and when you need more power or convenience, there's an ecosystem of thoughtfully designed packages waiting for you.

So do I need to write *Data Science from Scratch in R*? Maybe not. Because in R, you're never starting from scratch—you're starting from a foundation specifically designed for statistical computing and data analysis. And that's exactly where you want to be.

